name: CI/CD Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]
    types: [ opened, synchronize, reopened ]

env:
  PYTHON_VERSION: '3.11'
  # Kubernetes-native environment variables
  KUBE_NAMESPACE: 'ci-${{ github.run_id }}'
  MONGODB_POD: 'mongodb-${{ github.run_id }}'
  MONGODB_PORT: '27017'
  # Container image settings
  TEST_IMAGE_TAG: 'speecher-test:${{ github.sha }}'
  TEST_IMAGE_LATEST: 'speecher-test:latest'
  
jobs:
  # Optimized containerized test job - builds and runs tests in container
  test-optimized:
    name: 🚀 Optimized Container Tests
    runs-on: [self-hosted, linux, x64, kubernetes]
    
    steps:
    - uses: actions/checkout@v4
    
    - name: 🔍 Check for cached test image in cluster
      id: cache-check
      run: |
        echo "Checking for cached test image in cluster..."
        # Check if image exists in cluster's containerd namespace
        if kubectl get nodes -o jsonpath='{.items[0].status.images[*].names[*]}' 2>/dev/null | grep -q "speecher-test:latest"; then
          echo "cached=true" >> $GITHUB_OUTPUT
          echo "✅ Found cached test image in cluster"
        else
          echo "cached=false" >> $GITHUB_OUTPUT
          echo "📦 No cached image found, will build fresh"
        fi
    
    - name: 🏗️ Build optimized test container with Kaniko
      run: |
        echo "🏗️ Building optimized test container with Kaniko in Kubernetes..."
        
        # Create build namespace
        kubectl create namespace build-${{ github.run_id }} || true
        
        # Create build context ConfigMap
        echo "📦 Creating build context..."
        kubectl create configmap build-context-${{ github.run_id }} \
          --from-file=. \
          --namespace=build-${{ github.run_id }} \
          --dry-run=client -o yaml | kubectl apply -f -
        
        # Kaniko build job
        # Build the complete manifest based on cache availability
        if [ "${{ steps.cache-check.outputs.cached }}" = "true" ]; then
          cat <<EOF | kubectl apply -f -
        apiVersion: batch/v1
        kind: Job
        metadata:
          name: kaniko-build-${{ github.run_id }}
          namespace: build-${{ github.run_id }}
        spec:
          template:
            spec:
              restartPolicy: Never
              containers:
              - name: kaniko
                image: gcr.io/kaniko-project/executor:latest
                args:
                - --dockerfile=/workspace/docker/test-optimized.Dockerfile
                - --context=/workspace
                - --destination=${{ env.TEST_IMAGE_TAG }}
                - --destination=${{ env.TEST_IMAGE_LATEST }}
                - --target=runtime
                - --cache=true
                - --cache-ttl=24h
                - --cache-repo=speecher-test
                volumeMounts:
                - name: build-context
                  mountPath: /workspace
                resources:
                  requests:
                    memory: "1Gi"
                    cpu: "500m"
                  limits:
                    memory: "2Gi"
                    cpu: "1000m"
              volumes:
              - name: build-context
                configMap:
                  name: build-context-${{ github.run_id }}
        EOF
        else
          cat <<EOF | kubectl apply -f -
        apiVersion: batch/v1
        kind: Job
        metadata:
          name: kaniko-build-${{ github.run_id }}
          namespace: build-${{ github.run_id }}
        spec:
          template:
            spec:
              restartPolicy: Never
              containers:
              - name: kaniko
                image: gcr.io/kaniko-project/executor:latest
                args:
                - --dockerfile=/workspace/docker/test-optimized.Dockerfile
                - --context=/workspace
                - --destination=${{ env.TEST_IMAGE_TAG }}
                - --destination=${{ env.TEST_IMAGE_LATEST }}
                - --target=runtime
                - --cache=true
                - --cache-ttl=24h
                volumeMounts:
                - name: build-context
                  mountPath: /workspace
                resources:
                  requests:
                    memory: "1Gi"
                    cpu: "500m"
                  limits:
                    memory: "2Gi"
                    cpu: "1000m"
              volumes:
              - name: build-context
                configMap:
                  name: build-context-${{ github.run_id }}
        EOF
        fi
        
        echo "⏳ Waiting for Kaniko build to complete..."
        kubectl wait --for=condition=complete job/kaniko-build-${{ github.run_id }} \
          --timeout=600s \
          --namespace=build-${{ github.run_id }} || {
          echo "❌ Kaniko build failed"
          kubectl logs job/kaniko-build-${{ github.run_id }} --namespace=build-${{ github.run_id }} --tail=50
          exit 1
        }
        
        echo "✅ Test container built successfully with Kaniko"
        
        # Cleanup build resources
        kubectl delete namespace build-${{ github.run_id }} --grace-period=30 &
    
    - name: 🏥 Verify container health
      run: |
        echo "🏥 Running container health check..."
        
        # Create test namespace
        kubectl create namespace test-health-${{ github.run_id }} || true
        
        # Run health check in Kubernetes
        kubectl run health-check-${{ github.run_id }} \
          --image=${{ env.TEST_IMAGE_TAG }} \
          --namespace=test-health-${{ github.run_id }} \
          --restart=Never \
          --command -- python /app/healthcheck.py
        
        # Wait for pod to complete
        echo "⏳ Waiting for health check to complete..."
        for i in {1..30}; do
          POD_STATUS=$(kubectl get pod health-check-${{ github.run_id }} \
            --namespace=test-health-${{ github.run_id }} \
            -o jsonpath='{.status.phase}' 2>/dev/null || echo "Unknown")
          
          if [ "$POD_STATUS" = "Succeeded" ]; then
            echo "✅ Health check completed successfully"
            break
          elif [ "$POD_STATUS" = "Failed" ] || [ "$POD_STATUS" = "Error" ]; then
            echo "❌ Health check failed"
            kubectl logs health-check-${{ github.run_id }} --namespace=test-health-${{ github.run_id }}
            kubectl delete namespace test-health-${{ github.run_id }} --ignore-not-found=true
            exit 1
          fi
          
          echo "⏳ Health check status: $POD_STATUS ($i/30)"
          sleep 1
        done
        
        # Check final status
        POD_STATUS=$(kubectl get pod health-check-${{ github.run_id }} \
          --namespace=test-health-${{ github.run_id }} \
          -o jsonpath='{.status.phase}' 2>/dev/null || echo "Unknown")
        
        if [ "$POD_STATUS" != "Succeeded" ]; then
          echo "❌ Health check did not complete successfully (Status: $POD_STATUS)"
          kubectl logs health-check-${{ github.run_id }} --namespace=test-health-${{ github.run_id }} || true
          kubectl delete namespace test-health-${{ github.run_id }} --ignore-not-found=true
          exit 1
        fi
        
        echo "✅ Container health check passed"
        kubectl delete namespace test-health-${{ github.run_id }} --ignore-not-found=true
    
    - name: 🗃️ Deploy MongoDB for tests
      run: |
        echo "🚀 Creating test namespace..."
        kubectl create namespace ${{ env.KUBE_NAMESPACE }} || true
        
        echo "🗃️ Deploying MongoDB pod..."
        cat <<EOF | kubectl apply -f -
        apiVersion: v1
        kind: Pod
        metadata:
          name: ${{ env.MONGODB_POD }}
          namespace: ${{ env.KUBE_NAMESPACE }}
          labels:
            app: mongodb
            test-run: "${{ github.run_id }}"
        spec:
          containers:
          - name: mongodb
            image: mongo:6.0
            ports:
            - containerPort: 27017
            env:
            - name: MONGO_INITDB_ROOT_USERNAME
              value: root
            - name: MONGO_INITDB_ROOT_PASSWORD
              value: example
            resources:
              requests:
                memory: "256Mi"
                cpu: "250m"
              limits:
                memory: "512Mi"
                cpu: "500m"
        EOF
        
        echo "⏳ Waiting for MongoDB to be ready..."
        kubectl wait --for=condition=ready pod/${{ env.MONGODB_POD }} \
          --timeout=60s \
          --namespace=${{ env.KUBE_NAMESPACE }}
        
        echo "✅ MongoDB is ready"
    
    - name: 🧪 Run tests in Kubernetes
      id: run-tests
      run: |
        echo "🧪 Deploying test pod with MongoDB connection..."
        
        # Create ConfigMap for test configuration
        cat <<EOF | kubectl apply -f -
        apiVersion: v1
        kind: ConfigMap
        metadata:
          name: test-config-${{ github.run_id }}
          namespace: ${{ env.KUBE_NAMESPACE }}
        data:
          MONGODB_URI: "mongodb://root:example@${{ env.MONGODB_POD }}:27017"
          S3_BUCKET_NAME: "test-bucket"
          AZURE_STORAGE_ACCOUNT: "test-account"
          GCP_PROJECT_ID: "test-project"
          ENVIRONMENT: "test"
          PYTHONPATH: "/app"
        EOF
        
        # Deploy test Job
        cat <<EOF | kubectl apply -f -
        apiVersion: batch/v1
        kind: Job
        metadata:
          name: test-runner-${{ github.run_id }}
          namespace: ${{ env.KUBE_NAMESPACE }}
        spec:
          backoffLimit: 0
          activeDeadlineSeconds: 600
          template:
            metadata:
              labels:
                app: test-runner
                test-run: "${{ github.run_id }}"
            spec:
              restartPolicy: Never
              containers:
              - name: test-runner
                image: ${{ env.TEST_IMAGE_TAG }}
                command: ["/app/run_tests.sh"]
                args: ["all", "auto", "verbose"]
                envFrom:
                - configMapRef:
                    name: test-config-${{ github.run_id }}
                volumeMounts:
                - name: test-results
                  mountPath: /app/test_results
                - name: coverage
                  mountPath: /app/coverage
                resources:
                  requests:
                    memory: "512Mi"
                    cpu: "500m"
                  limits:
                    memory: "1Gi"
                    cpu: "1000m"
              volumes:
              - name: test-results
                emptyDir: {}
              - name: coverage
                emptyDir: {}
        EOF
        
        echo "⏳ Waiting for tests to complete..."
        kubectl wait --for=condition=complete job/test-runner-${{ github.run_id }} \
          --timeout=600s \
          --namespace=${{ env.KUBE_NAMESPACE }} || {
            echo "❌ Tests failed or timed out"
            echo "📋 Test logs:"
            kubectl logs job/test-runner-${{ github.run_id }} --namespace=${{ env.KUBE_NAMESPACE }} --tail=100
            echo "test_status=failed" >> $GITHUB_OUTPUT
            exit 1
          }
        
        echo "✅ Tests completed successfully"
        echo "test_status=success" >> $GITHUB_OUTPUT
        
        # Get test logs for artifact
        kubectl logs job/test-runner-${{ github.run_id }} --namespace=${{ env.KUBE_NAMESPACE }} > test-output.log
    
    - name: 📊 Extract test results
      if: always()
      run: |
        echo "📊 Extracting test results from container..."
        
        # Get the pod name from the job
        POD_NAME=$(kubectl get pods \
          --namespace=${{ env.KUBE_NAMESPACE }} \
          -l app=test-runner,test-run=${{ github.run_id }} \
          -o jsonpath='{.items[0].metadata.name}')
        
        if [ -n "$POD_NAME" ]; then
          echo "📦 Found test pod: $POD_NAME"
          
          # Create local directories for results
          mkdir -p ./test_results ./coverage_reports
          
          # Copy test results
          kubectl cp ${{ env.KUBE_NAMESPACE }}/$POD_NAME:/app/test_results ./test_results || true
          kubectl cp ${{ env.KUBE_NAMESPACE }}/$POD_NAME:/app/coverage ./coverage_reports || true
          
          # Check if files were copied
          if [ -f "./test_results/junit.xml" ]; then
            echo "✅ JUnit results extracted"
          fi
          
          if [ -f "./test_results/coverage.xml" ]; then
            echo "✅ Coverage XML extracted"
            # Display coverage summary
            python -c "
        import xml.etree.ElementTree as ET
        try:
            tree = ET.parse('./test_results/coverage.xml')
            root = tree.getroot()
            coverage = float(root.attrib.get('line-rate', 0)) * 100
            print(f'📈 Overall test coverage: {coverage:.2f}%')
        except Exception as e:
            print(f'Could not parse coverage: {e}')
        " || true
          fi
        else
          echo "⚠️ Could not find test pod for result extraction"
        fi
    
    - name: 📤 Upload test artifacts
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: test-results-optimized
        path: |
          test-output.log
          test_results/
          coverage_reports/
        retention-days: 30
    
    - name: 📈 Upload coverage to Codecov
      if: always() && steps.run-tests.outputs.test_status == 'success'
      uses: codecov/codecov-action@v3
      with:
        file: ./test_results/coverage.xml
        flags: optimized-tests
        name: codecov-optimized
        fail_ci_if_error: false
    
    - name: 🧹 Cleanup Kubernetes resources
      if: always()
      run: |
        echo "🧹 Cleaning up Kubernetes resources..."
        
        # Delete namespace (removes all resources within it)
        kubectl delete namespace ${{ env.KUBE_NAMESPACE }} --ignore-not-found=true --grace-period=30 &
        
        echo "✅ Cleanup initiated (namespace deletion continues in background)"
    
    - name: 🏷️ Tag image for cache (on success)
      if: success()
      run: |
        echo "🏷️ Tagging successful test image for cache..."
        # Image is already tagged as latest during build
        echo "✅ Image cached as ${{ env.TEST_IMAGE_LATEST }}"

  # Original test job - runs tests directly with Python and MongoDB port-forwarding
  # Kept for compatibility and as a fallback path while test-optimized stabilizes
  test:
    name: Run Tests
    runs-on: [self-hosted, linux, x64, kubernetes]
    
    # Using Kubernetes-native service deployment instead of Docker services
    # This approach works with containerd runners that don't have Docker daemon
    
    steps:
    - uses: actions/checkout@v4
    
    - name: 🗃️ Setup MongoDB with Kubernetes (containerd-compatible)
      run: |
        echo "🚀 Creating dedicated namespace for CI run..."
        kubectl create namespace ${{ env.KUBE_NAMESPACE }} || kubectl get namespace ${{ env.KUBE_NAMESPACE }}
        
        echo "🗃️ Deploying MongoDB pod..."
        kubectl run ${{ env.MONGODB_POD }} \
          --image=mongo:6.0 \
          --port=${{ env.MONGODB_PORT }} \
          --namespace=${{ env.KUBE_NAMESPACE }} \
          --env="MONGO_INITDB_ROOT_USERNAME=root" \
          --env="MONGO_INITDB_ROOT_PASSWORD=example" \
          --restart=Never
        
        echo "⏳ Waiting for MongoDB pod to be ready..."
        kubectl wait --for=condition=ready pod/${{ env.MONGODB_POD }} \
          --timeout=120s \
          --namespace=${{ env.KUBE_NAMESPACE }} || (
            echo "⚠️ MongoDB pod not ready after 120s, checking status..."
            kubectl describe pod ${{ env.MONGODB_POD }} --namespace=${{ env.KUBE_NAMESPACE }}
            kubectl logs ${{ env.MONGODB_POD }} --namespace=${{ env.KUBE_NAMESPACE }} || true
            exit 1
          )
        
        echo "🔗 Setting up port forwarding with retry logic..."
        PORT_FORWARD_PID=""
        MAX_RETRIES=5
        RETRY_COUNT=0
        
        while [ $RETRY_COUNT -lt $MAX_RETRIES ]; do
          echo "📡 Port forwarding attempt $((RETRY_COUNT + 1))/$MAX_RETRIES..."
          
          # Kill any existing port-forward process for this pod
          pkill -f "kubectl port-forward.*${{ env.MONGODB_POD }}.*${{ env.KUBE_NAMESPACE }}" || true
          sleep 2
          
          # Start port forwarding in background and capture PID
          kubectl port-forward pod/${{ env.MONGODB_POD }} \
            ${{ env.MONGODB_PORT }}:${{ env.MONGODB_PORT }} \
            --namespace=${{ env.KUBE_NAMESPACE }} &
          PORT_FORWARD_PID=$!
          
          # Store PID for cleanup
          echo $PORT_FORWARD_PID > /tmp/mongodb-port-forward-${{ github.run_id }}.pid
          
          # Wait a moment for port forwarding to establish
          sleep 3
          
          # Check if port-forward process is still running
          if ! kill -0 $PORT_FORWARD_PID 2>/dev/null; then
            echo "⚠️ Port forward process died immediately, checking pod status..."
            kubectl get pod ${{ env.MONGODB_POD }} --namespace=${{ env.KUBE_NAMESPACE }}
            kubectl logs ${{ env.MONGODB_POD }} --namespace=${{ env.KUBE_NAMESPACE }} --tail=20
            RETRY_COUNT=$((RETRY_COUNT + 1))
            continue
          fi
          
          # Verify MongoDB is accessible with timeout
          echo "🔍 Verifying MongoDB connection on localhost:${{ env.MONGODB_PORT }}..."
          CONNECTION_SUCCESS=false
          
          for i in {1..30}; do
            if nc -z -w 1 localhost ${{ env.MONGODB_PORT }} 2>/dev/null; then
              echo "✅ MongoDB port ${{ env.MONGODB_PORT }} is accessible!"
              CONNECTION_SUCCESS=true
              break
            fi
            echo "⏳ Waiting for MongoDB connection... ($i/30)"
            sleep 1
          done
          
          if [ "$CONNECTION_SUCCESS" = true ]; then
            echo "🎉 MongoDB setup completed successfully!"
            echo "📌 Port forward PID: $PORT_FORWARD_PID"
            
            # Double-check with MongoDB client if possible
            if command -v mongosh &> /dev/null; then
              mongosh --eval "db.version()" mongodb://localhost:${{ env.MONGODB_PORT }} --quiet || true
            fi
            break
          else
            echo "❌ Failed to connect to MongoDB on attempt $((RETRY_COUNT + 1))"
            # Kill the failed port-forward process
            kill $PORT_FORWARD_PID 2>/dev/null || true
            RETRY_COUNT=$((RETRY_COUNT + 1))
            
            if [ $RETRY_COUNT -lt $MAX_RETRIES ]; then
              echo "⏳ Waiting 5 seconds before retry..."
              sleep 5
            fi
          fi
        done
        
        if [ $RETRY_COUNT -eq $MAX_RETRIES ]; then
          echo "❌ Failed to establish MongoDB port forwarding after $MAX_RETRIES attempts"
          echo "📋 Final diagnostics:"
          kubectl get pod ${{ env.MONGODB_POD }} --namespace=${{ env.KUBE_NAMESPACE }}
          kubectl describe pod ${{ env.MONGODB_POD }} --namespace=${{ env.KUBE_NAMESPACE }}
          kubectl logs ${{ env.MONGODB_POD }} --namespace=${{ env.KUBE_NAMESPACE }} --tail=50
          exit 1
        fi
        
        # Final verification
        echo "🔒 Final MongoDB connectivity check..."
        timeout 10 bash -c 'while ! nc -z localhost ${{ env.MONGODB_PORT }}; do sleep 0.5; done' || {
          echo "❌ Final verification failed - MongoDB not accessible"
          exit 1
        }
        echo "✅ MongoDB is ready and accessible on localhost:${{ env.MONGODB_PORT }}"
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Cache dependencies
      uses: actions/cache@v4
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt', '**/pyproject.toml') }}
        restore-keys: |
          ${{ runner.os }}-pip-
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements/base.txt
        pip install -r requirements/test.txt
    
    - name: Run unit tests
      env:
        MONGODB_URI: mongodb://localhost:27017
        S3_BUCKET_NAME: test-bucket
        AZURE_STORAGE_ACCOUNT: test-account
        GCP_PROJECT_ID: test-project
      run: |
        pytest tests/test_api.py -v --cov=src/backend --cov-report=xml
    
    - name: Run integration tests
      env:
        MONGODB_URI: mongodb://localhost:27017
      run: |
        pytest tests/test_integration.py -v
    
    - name: 🧹 Cleanup Kubernetes Resources
      if: always()
      run: |
        echo "🧹 Cleaning up Kubernetes resources..."
        
        # Kill port-forward process using stored PID
        if [ -f /tmp/mongodb-port-forward-${{ github.run_id }}.pid ]; then
          PID=$(cat /tmp/mongodb-port-forward-${{ github.run_id }}.pid)
          echo "📌 Killing port-forward process with PID: $PID"
          kill $PID 2>/dev/null || true
          rm -f /tmp/mongodb-port-forward-${{ github.run_id }}.pid
        fi
        
        # Also kill any remaining port-forward processes for this namespace (fallback)
        pkill -f "kubectl port-forward.*${{ env.MONGODB_POD }}.*${{ env.KUBE_NAMESPACE }}" || true
        
        # Clean up MongoDB pod
        echo "🗑️ Deleting MongoDB pod..."
        kubectl delete pod ${{ env.MONGODB_POD }} \
          --namespace=${{ env.KUBE_NAMESPACE }} \
          --ignore-not-found=true \
          --grace-period=5
        
        # Clean up namespace (this removes all resources in the namespace)
        echo "🗑️ Deleting namespace ${{ env.KUBE_NAMESPACE }}..."
        kubectl delete namespace ${{ env.KUBE_NAMESPACE }} --ignore-not-found=true &
        
        echo "✅ Cleanup initiated (namespace deletion continues in background)"
    
    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        flags: unittests
        name: codecov-umbrella
        fail_ci_if_error: false

  lint:
    name: Lint Code
    runs-on: [self-hosted, linux, x64, kubernetes]
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install linting tools
      run: |
        python -m pip install --upgrade pip
        pip install flake8 black isort mypy
    
    - name: Run Black
      run: black --check src/ tests/
      continue-on-error: true
    
    - name: Run isort
      run: isort --check-only src/ tests/
      continue-on-error: true
    
    - name: Run Flake8
      run: flake8 src/ tests/ --max-line-length=120 --ignore=E203,W503
      continue-on-error: true
    
    - name: Run MyPy
      run: mypy src/ --ignore-missing-imports
      continue-on-error: true

  # Container build using Kubernetes Jobs with Kaniko
  container-build:
    name: 🐳 Build Container Images (Kubernetes)
    runs-on: [self-hosted, linux, x64, kubernetes]
    
    steps:
    - uses: actions/checkout@v4
    
    - name: 🏗️ Build Backend Image with Kaniko
      run: |
        echo "🏗️ Building backend image with Kaniko in Kubernetes..."
        
        # Create dedicated build namespace
        kubectl create namespace container-build-${{ github.run_id }} || true
        
        # Create build context from current directory
        echo "📦 Preparing build context..."
        kubectl create configmap backend-build-context \
          --from-file=. \
          --namespace=container-build-${{ github.run_id }} \
          --dry-run=client -o yaml | kubectl apply -f -
        
        # Backend build job with Kaniko
        cat <<EOF | kubectl apply -f -
        apiVersion: batch/v1
        kind: Job
        metadata:
          name: backend-build-${{ github.run_id }}
          namespace: container-build-${{ github.run_id }}
        spec:
          template:
            spec:
              restartPolicy: Never
              containers:
              - name: kaniko
                image: gcr.io/kaniko-project/executor:latest
                args:
                - --dockerfile=/workspace/Dockerfile
                - --context=/workspace
                - --destination=speecher-backend:ci-${{ github.run_id }}
                - --cache=true
                - --cache-ttl=24h
                volumeMounts:
                - name: build-context
                  mountPath: /workspace
                resources:
                  requests:
                    memory: "1Gi"
                    cpu: "500m"
                  limits:
                    memory: "2Gi"
                    cpu: "1000m"
              volumes:
              - name: build-context
                configMap:
                  name: backend-build-context
        EOF
        
        echo "⏳ Waiting for backend build to complete..."
        kubectl wait --for=condition=complete job/backend-build-${{ github.run_id }} \
          --timeout=600s \
          --namespace=container-build-${{ github.run_id }} || {
          echo "❌ Backend build failed"
          kubectl logs job/backend-build-${{ github.run_id }} --namespace=container-build-${{ github.run_id }} --tail=50
          exit 1
        }
        
        echo "✅ Backend image built successfully"
    
    - name: 🏗️ Build Frontend Image with Kaniko
      run: |
        echo "🏗️ Building frontend image with Kaniko..."
        
        if [ -f "docker/react.Dockerfile" ]; then
          # Frontend build job
          cat <<EOF | kubectl apply -f -
        apiVersion: batch/v1
        kind: Job
        metadata:
          name: frontend-build-${{ github.run_id }}
          namespace: container-build-${{ github.run_id }}
        spec:
          template:
            spec:
              restartPolicy: Never
              containers:
              - name: kaniko
                image: gcr.io/kaniko-project/executor:latest
                args:
                - --dockerfile=/workspace/docker/react.Dockerfile
                - --context=/workspace
                - --destination=speecher-frontend:ci-${{ github.run_id }}
                - --cache=true
                - --cache-ttl=24h
                volumeMounts:
                - name: build-context
                  mountPath: /workspace
                resources:
                  requests:
                    memory: "1Gi"
                    cpu: "500m"
                  limits:
                    memory: "2Gi"
                    cpu: "1000m"
              volumes:
              - name: build-context
                configMap:
                  name: backend-build-context
        EOF
          
          echo "⏳ Waiting for frontend build to complete..."
          kubectl wait --for=condition=complete job/frontend-build-${{ github.run_id }} \
            --timeout=600s \
            --namespace=container-build-${{ github.run_id }} || {
            echo "❌ Frontend build failed"
            kubectl logs job/frontend-build-${{ github.run_id }} --namespace=container-build-${{ github.run_id }} --tail=50
            echo "⚠️ Frontend build failed, continuing..."
          }
        else
          echo "⚠️ Frontend Dockerfile not found, skipping frontend build"
        fi
        
        echo "✅ Frontend build completed"
    
    - name: 🧪 Test Container Functionality
      run: |
        echo "🧪 Testing backend container built with Kaniko..."
        
        # Create test namespace
        kubectl create namespace test-containers-${{ github.run_id }} || true
        
        # Deploy backend for testing
        cat <<EOF | kubectl apply -f -
        apiVersion: v1
        kind: Pod
        metadata:
          name: backend-test
          namespace: test-containers-${{ github.run_id }}
          labels:
            app: backend-test
        spec:
          restartPolicy: Never
          containers:
          - name: backend
            image: speecher-backend:ci-${{ github.run_id }}
            imagePullPolicy: Never
            ports:
            - containerPort: 8000
            env:
            - name: ENVIRONMENT
              value: "test"
            resources:
              requests:
                memory: "256Mi"
                cpu: "100m"
              limits:
                memory: "512Mi"
                cpu: "500m"
        EOF
        
        # Wait for pod to be ready
        echo "⏳ Waiting for backend pod to be ready..."
        kubectl wait --for=condition=ready pod/backend-test \
          --timeout=120s \
          --namespace=test-containers-${{ github.run_id }} || {
            echo "⚠️ Backend pod not ready, checking status..."
            kubectl get pods --namespace=test-containers-${{ github.run_id }}
            kubectl describe pod/backend-test --namespace=test-containers-${{ github.run_id }}
            kubectl logs backend-test --namespace=test-containers-${{ github.run_id }} --tail=20 || true
          }
        
        # Test container health using kubectl exec instead of port forwarding
        echo "🔍 Testing backend container health..."
        kubectl exec backend-test \
          --namespace=test-containers-${{ github.run_id }} \
          -- python --version || echo "⚠️ Basic python test failed"
        
        echo "✅ Container functionality test completed"
        
        # Cleanup
        kubectl delete namespace test-containers-${{ github.run_id }} --ignore-not-found=true &
        kubectl delete namespace container-build-${{ github.run_id }} --ignore-not-found=true &

  security:
    name: Security Scan
    runs-on: [self-hosted, linux, x64, kubernetes]
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install security tools
      run: |
        python -m pip install --upgrade pip
        pip install safety bandit
    
    - name: Run Safety check
      run: |
        pip install -r requirements/base.txt || true
        safety check || true
      continue-on-error: true
    
    - name: Run Bandit
      run: bandit -r src/ -f json -o bandit-report.json
      continue-on-error: true
    
    - name: Upload security reports
      uses: actions/upload-artifact@v4
      with:
        name: security-reports
        path: |
          bandit-report.json
        retention-days: 30

  deploy:
    name: 🚀 Deploy to Production
    needs: [test-optimized, test, lint, security, container-build]  # Includes both test paths
    runs-on: [self-hosted, linux, x64, kubernetes]
    if: github.ref == 'refs/heads/main' && github.event_name == 'push'
    
    steps:
    - uses: actions/checkout@v4
    
    - name: 🚀 Deploy Notification
      run: |
        echo "🎉 Ready for deployment!"
        echo "📦 Container images built: speecher-backend:ci-${{ github.run_id }}"
        echo "🔧 This is where you would deploy to production"
        echo "💡 Add your K8s deployment steps here (kubectl apply, Helm, ArgoCD, etc.)"
    
    # Kubernetes-native deployment examples (uncomment and configure)
    # - name: 🚀 Deploy to Kubernetes
    #   run: |
    #     # Images are already built and available in cluster via Kaniko
    #     # Deploy using kubectl
    #     # kubectl apply -f k8s/production/
    #     
    #     # Or deploy using Helm
    #     # helm upgrade --install speecher ./helm-chart --namespace=production \
    #     #   --set image.tag=ci-${{ github.run_id }}
    
    # - name: 🐳 Push to External Container Registry
    #   env:
    #     REGISTRY_USERNAME: ${{ secrets.REGISTRY_USERNAME }}
    #     REGISTRY_PASSWORD: ${{ secrets.REGISTRY_PASSWORD }}
    #   run: |
    #     # Use Kaniko to build and push directly to external registry
    #     cat <<EOF | kubectl apply -f -
    #     apiVersion: batch/v1
    #     kind: Job
    #     metadata:
    #       name: registry-push-${{ github.run_id }}
    #       namespace: default
    #     spec:
    #       template:
    #         spec:
    #           restartPolicy: Never
    #           containers:
    #           - name: kaniko
    #             image: gcr.io/kaniko-project/executor:latest
    #             args:
    #             - --dockerfile=/workspace/Dockerfile
    #             - --context=/workspace
    #             - --destination=your-registry.com/speecher-backend:latest
    #             - --destination=your-registry.com/speecher-backend:ci-${{ github.run_id }}
    #             volumeMounts:
    #             - name: docker-config
    #               mountPath: /kaniko/.docker
    #           volumes:
    #           - name: docker-config
    #             secret:
    #               secretName: docker-registry-secret
    #     EOF