name: CI/CD Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]
    types: [ opened, synchronize, reopened ]

env:
  PYTHON_VERSION: '3.11'
  # Kubernetes-native environment variables
  KUBE_NAMESPACE: 'ci-${{ github.run_id }}'
  MONGODB_POD: 'mongodb-${{ github.run_id }}'
  MONGODB_PORT: '27017'
  # Container image settings
  TEST_IMAGE_TAG: 'speecher-test:${{ github.sha }}'
  TEST_IMAGE_LATEST: 'speecher-test:latest'
  
jobs:
  # Optimized containerized test job - builds and runs tests in container
  test-optimized:
    name: ğŸš€ Optimized Container Tests
    runs-on: [self-hosted, containerd]
    
    steps:
    - uses: actions/checkout@v4
    
    - name: ğŸ” Check for cached test image
      id: cache-check
      run: |
        echo "Checking for cached test image..."
        if nerdctl images | grep -q "speecher-test.*latest"; then
          echo "cached=true" >> $GITHUB_OUTPUT
          echo "âœ… Found cached test image"
          nerdctl images | grep speecher-test
        else
          echo "cached=false" >> $GITHUB_OUTPUT
          echo "ğŸ“¦ No cached image found, will build fresh"
        fi
    
    - name: ğŸ—ï¸ Build optimized test container
      run: |
        echo "ğŸ—ï¸ Building optimized test container with nerdctl..."
        
        # Build with cache from latest if available
        if [ "${{ steps.cache-check.outputs.cached }}" = "true" ]; then
          echo "ğŸ“¦ Using cached layers from speecher-test:latest..."
          nerdctl build \
            --cache-from ${{ env.TEST_IMAGE_LATEST }} \
            --build-arg BUILDKIT_INLINE_CACHE=1 \
            --target runtime \
            -f docker/test-optimized.Dockerfile \
            -t ${{ env.TEST_IMAGE_TAG }} \
            -t ${{ env.TEST_IMAGE_LATEST }} \
            .
        else
          echo "ğŸ”¨ Building fresh test container..."
          nerdctl build \
            --build-arg BUILDKIT_INLINE_CACHE=1 \
            --target runtime \
            -f docker/test-optimized.Dockerfile \
            -t ${{ env.TEST_IMAGE_TAG }} \
            -t ${{ env.TEST_IMAGE_LATEST }} \
            .
        fi
        
        echo "âœ… Test container built successfully"
        echo "ğŸ“‹ Image details:"
        nerdctl images | grep speecher-test
    
    - name: ğŸ¥ Verify container health
      run: |
        echo "ğŸ¥ Running container health check..."
        
        # Create test namespace
        kubectl create namespace test-health-${{ github.run_id }} || true
        
        # Run health check in Kubernetes
        kubectl run health-check-${{ github.run_id }} \
          --image=${{ env.TEST_IMAGE_TAG }} \
          --namespace=test-health-${{ github.run_id }} \
          --restart=Never \
          --command -- python /app/healthcheck.py
        
        # Wait for pod to complete
        echo "â³ Waiting for health check to complete..."
        for i in {1..30}; do
          POD_STATUS=$(kubectl get pod health-check-${{ github.run_id }} \
            --namespace=test-health-${{ github.run_id }} \
            -o jsonpath='{.status.phase}' 2>/dev/null || echo "Unknown")
          
          if [ "$POD_STATUS" = "Succeeded" ]; then
            echo "âœ… Health check completed successfully"
            break
          elif [ "$POD_STATUS" = "Failed" ] || [ "$POD_STATUS" = "Error" ]; then
            echo "âŒ Health check failed"
            kubectl logs health-check-${{ github.run_id }} --namespace=test-health-${{ github.run_id }}
            kubectl delete namespace test-health-${{ github.run_id }} --ignore-not-found=true
            exit 1
          fi
          
          echo "â³ Health check status: $POD_STATUS ($i/30)"
          sleep 1
        done
        
        # Check final status
        POD_STATUS=$(kubectl get pod health-check-${{ github.run_id }} \
          --namespace=test-health-${{ github.run_id }} \
          -o jsonpath='{.status.phase}' 2>/dev/null || echo "Unknown")
        
        if [ "$POD_STATUS" != "Succeeded" ]; then
          echo "âŒ Health check did not complete successfully (Status: $POD_STATUS)"
          kubectl logs health-check-${{ github.run_id }} --namespace=test-health-${{ github.run_id }} || true
          kubectl delete namespace test-health-${{ github.run_id }} --ignore-not-found=true
          exit 1
        fi
        
        echo "âœ… Container health check passed"
        kubectl delete namespace test-health-${{ github.run_id }} --ignore-not-found=true
    
    - name: ğŸ—ƒï¸ Deploy MongoDB for tests
      run: |
        echo "ğŸš€ Creating test namespace..."
        kubectl create namespace ${{ env.KUBE_NAMESPACE }} || true
        
        echo "ğŸ—ƒï¸ Deploying MongoDB pod..."
        cat <<EOF | kubectl apply -f -
        apiVersion: v1
        kind: Pod
        metadata:
          name: ${{ env.MONGODB_POD }}
          namespace: ${{ env.KUBE_NAMESPACE }}
          labels:
            app: mongodb
            test-run: "${{ github.run_id }}"
        spec:
          containers:
          - name: mongodb
            image: mongo:6.0
            ports:
            - containerPort: 27017
            env:
            - name: MONGO_INITDB_ROOT_USERNAME
              value: root
            - name: MONGO_INITDB_ROOT_PASSWORD
              value: example
            resources:
              requests:
                memory: "256Mi"
                cpu: "250m"
              limits:
                memory: "512Mi"
                cpu: "500m"
        EOF
        
        echo "â³ Waiting for MongoDB to be ready..."
        kubectl wait --for=condition=ready pod/${{ env.MONGODB_POD }} \
          --timeout=60s \
          --namespace=${{ env.KUBE_NAMESPACE }}
        
        echo "âœ… MongoDB is ready"
    
    - name: ğŸ§ª Run tests in Kubernetes
      id: run-tests
      run: |
        echo "ğŸ§ª Deploying test pod with MongoDB connection..."
        
        # Create ConfigMap for test configuration
        cat <<EOF | kubectl apply -f -
        apiVersion: v1
        kind: ConfigMap
        metadata:
          name: test-config-${{ github.run_id }}
          namespace: ${{ env.KUBE_NAMESPACE }}
        data:
          MONGODB_URI: "mongodb://root:example@${{ env.MONGODB_POD }}:27017"
          S3_BUCKET_NAME: "test-bucket"
          AZURE_STORAGE_ACCOUNT: "test-account"
          GCP_PROJECT_ID: "test-project"
          ENVIRONMENT: "test"
          PYTHONPATH: "/app"
        EOF
        
        # Deploy test Job
        cat <<EOF | kubectl apply -f -
        apiVersion: batch/v1
        kind: Job
        metadata:
          name: test-runner-${{ github.run_id }}
          namespace: ${{ env.KUBE_NAMESPACE }}
        spec:
          backoffLimit: 0
          activeDeadlineSeconds: 600
          template:
            metadata:
              labels:
                app: test-runner
                test-run: "${{ github.run_id }}"
            spec:
              restartPolicy: Never
              containers:
              - name: test-runner
                image: ${{ env.TEST_IMAGE_TAG }}
                command: ["/app/run_tests.sh"]
                args: ["all", "auto", "verbose"]
                envFrom:
                - configMapRef:
                    name: test-config-${{ github.run_id }}
                volumeMounts:
                - name: test-results
                  mountPath: /app/test_results
                - name: coverage
                  mountPath: /app/coverage
                resources:
                  requests:
                    memory: "512Mi"
                    cpu: "500m"
                  limits:
                    memory: "1Gi"
                    cpu: "1000m"
              volumes:
              - name: test-results
                emptyDir: {}
              - name: coverage
                emptyDir: {}
        EOF
        
        echo "â³ Waiting for tests to complete..."
        kubectl wait --for=condition=complete job/test-runner-${{ github.run_id }} \
          --timeout=600s \
          --namespace=${{ env.KUBE_NAMESPACE }} || {
            echo "âŒ Tests failed or timed out"
            echo "ğŸ“‹ Test logs:"
            kubectl logs job/test-runner-${{ github.run_id }} --namespace=${{ env.KUBE_NAMESPACE }} --tail=100
            echo "test_status=failed" >> $GITHUB_OUTPUT
            exit 1
          }
        
        echo "âœ… Tests completed successfully"
        echo "test_status=success" >> $GITHUB_OUTPUT
        
        # Get test logs for artifact
        kubectl logs job/test-runner-${{ github.run_id }} --namespace=${{ env.KUBE_NAMESPACE }} > test-output.log
    
    - name: ğŸ“Š Extract test results
      if: always()
      run: |
        echo "ğŸ“Š Extracting test results from container..."
        
        # Get the pod name from the job
        POD_NAME=$(kubectl get pods \
          --namespace=${{ env.KUBE_NAMESPACE }} \
          -l app=test-runner,test-run=${{ github.run_id }} \
          -o jsonpath='{.items[0].metadata.name}')
        
        if [ -n "$POD_NAME" ]; then
          echo "ğŸ“¦ Found test pod: $POD_NAME"
          
          # Create local directories for results
          mkdir -p ./test_results ./coverage_reports
          
          # Copy test results
          kubectl cp ${{ env.KUBE_NAMESPACE }}/$POD_NAME:/app/test_results ./test_results || true
          kubectl cp ${{ env.KUBE_NAMESPACE }}/$POD_NAME:/app/coverage ./coverage_reports || true
          
          # Check if files were copied
          if [ -f "./test_results/junit.xml" ]; then
            echo "âœ… JUnit results extracted"
          fi
          
          if [ -f "./test_results/coverage.xml" ]; then
            echo "âœ… Coverage XML extracted"
            # Display coverage summary
            python -c "
        import xml.etree.ElementTree as ET
        try:
            tree = ET.parse('./test_results/coverage.xml')
            root = tree.getroot()
            coverage = float(root.attrib.get('line-rate', 0)) * 100
            print(f'ğŸ“ˆ Overall test coverage: {coverage:.2f}%')
        except Exception as e:
            print(f'Could not parse coverage: {e}')
        " || true
          fi
        else
          echo "âš ï¸ Could not find test pod for result extraction"
        fi
    
    - name: ğŸ“¤ Upload test artifacts
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: test-results-optimized
        path: |
          test-output.log
          test_results/
          coverage_reports/
        retention-days: 30
    
    - name: ğŸ“ˆ Upload coverage to Codecov
      if: always() && steps.run-tests.outputs.test_status == 'success'
      uses: codecov/codecov-action@v3
      with:
        file: ./test_results/coverage.xml
        flags: optimized-tests
        name: codecov-optimized
        fail_ci_if_error: false
    
    - name: ğŸ§¹ Cleanup Kubernetes resources
      if: always()
      run: |
        echo "ğŸ§¹ Cleaning up Kubernetes resources..."
        
        # Delete namespace (removes all resources within it)
        kubectl delete namespace ${{ env.KUBE_NAMESPACE }} --ignore-not-found=true --grace-period=30 &
        
        echo "âœ… Cleanup initiated (namespace deletion continues in background)"
    
    - name: ğŸ·ï¸ Tag image for cache (on success)
      if: success()
      run: |
        echo "ğŸ·ï¸ Tagging successful test image for cache..."
        # Image is already tagged as latest during build
        echo "âœ… Image cached as ${{ env.TEST_IMAGE_LATEST }}"

  # Original test job - runs tests directly with Python and MongoDB port-forwarding
  # Kept for compatibility and as a fallback path while test-optimized stabilizes
  test:
    name: Run Tests
    runs-on: [self-hosted, containerd]
    
    # Using Kubernetes-native service deployment instead of Docker services
    # This approach works with containerd runners that don't have Docker daemon
    
    steps:
    - uses: actions/checkout@v4
    
    - name: ğŸ—ƒï¸ Setup MongoDB with Kubernetes (containerd-compatible)
      run: |
        echo "ğŸš€ Creating dedicated namespace for CI run..."
        kubectl create namespace ${{ env.KUBE_NAMESPACE }} || kubectl get namespace ${{ env.KUBE_NAMESPACE }}
        
        echo "ğŸ—ƒï¸ Deploying MongoDB pod..."
        kubectl run ${{ env.MONGODB_POD }} \
          --image=mongo:6.0 \
          --port=${{ env.MONGODB_PORT }} \
          --namespace=${{ env.KUBE_NAMESPACE }} \
          --env="MONGO_INITDB_ROOT_USERNAME=root" \
          --env="MONGO_INITDB_ROOT_PASSWORD=example" \
          --restart=Never
        
        echo "â³ Waiting for MongoDB pod to be ready..."
        kubectl wait --for=condition=ready pod/${{ env.MONGODB_POD }} \
          --timeout=120s \
          --namespace=${{ env.KUBE_NAMESPACE }} || (
            echo "âš ï¸ MongoDB pod not ready after 120s, checking status..."
            kubectl describe pod ${{ env.MONGODB_POD }} --namespace=${{ env.KUBE_NAMESPACE }}
            kubectl logs ${{ env.MONGODB_POD }} --namespace=${{ env.KUBE_NAMESPACE }} || true
            exit 1
          )
        
        echo "ğŸ”— Setting up port forwarding with retry logic..."
        PORT_FORWARD_PID=""
        MAX_RETRIES=5
        RETRY_COUNT=0
        
        while [ $RETRY_COUNT -lt $MAX_RETRIES ]; do
          echo "ğŸ“¡ Port forwarding attempt $((RETRY_COUNT + 1))/$MAX_RETRIES..."
          
          # Kill any existing port-forward process for this pod
          pkill -f "kubectl port-forward.*${{ env.MONGODB_POD }}.*${{ env.KUBE_NAMESPACE }}" || true
          sleep 2
          
          # Start port forwarding in background and capture PID
          kubectl port-forward pod/${{ env.MONGODB_POD }} \
            ${{ env.MONGODB_PORT }}:${{ env.MONGODB_PORT }} \
            --namespace=${{ env.KUBE_NAMESPACE }} &
          PORT_FORWARD_PID=$!
          
          # Store PID for cleanup
          echo $PORT_FORWARD_PID > /tmp/mongodb-port-forward-${{ github.run_id }}.pid
          
          # Wait a moment for port forwarding to establish
          sleep 3
          
          # Check if port-forward process is still running
          if ! kill -0 $PORT_FORWARD_PID 2>/dev/null; then
            echo "âš ï¸ Port forward process died immediately, checking pod status..."
            kubectl get pod ${{ env.MONGODB_POD }} --namespace=${{ env.KUBE_NAMESPACE }}
            kubectl logs ${{ env.MONGODB_POD }} --namespace=${{ env.KUBE_NAMESPACE }} --tail=20
            RETRY_COUNT=$((RETRY_COUNT + 1))
            continue
          fi
          
          # Verify MongoDB is accessible with timeout
          echo "ğŸ” Verifying MongoDB connection on localhost:${{ env.MONGODB_PORT }}..."
          CONNECTION_SUCCESS=false
          
          for i in {1..30}; do
            if nc -z -w 1 localhost ${{ env.MONGODB_PORT }} 2>/dev/null; then
              echo "âœ… MongoDB port ${{ env.MONGODB_PORT }} is accessible!"
              CONNECTION_SUCCESS=true
              break
            fi
            echo "â³ Waiting for MongoDB connection... ($i/30)"
            sleep 1
          done
          
          if [ "$CONNECTION_SUCCESS" = true ]; then
            echo "ğŸ‰ MongoDB setup completed successfully!"
            echo "ğŸ“Œ Port forward PID: $PORT_FORWARD_PID"
            
            # Double-check with MongoDB client if possible
            if command -v mongosh &> /dev/null; then
              mongosh --eval "db.version()" mongodb://localhost:${{ env.MONGODB_PORT }} --quiet || true
            fi
            break
          else
            echo "âŒ Failed to connect to MongoDB on attempt $((RETRY_COUNT + 1))"
            # Kill the failed port-forward process
            kill $PORT_FORWARD_PID 2>/dev/null || true
            RETRY_COUNT=$((RETRY_COUNT + 1))
            
            if [ $RETRY_COUNT -lt $MAX_RETRIES ]; then
              echo "â³ Waiting 5 seconds before retry..."
              sleep 5
            fi
          fi
        done
        
        if [ $RETRY_COUNT -eq $MAX_RETRIES ]; then
          echo "âŒ Failed to establish MongoDB port forwarding after $MAX_RETRIES attempts"
          echo "ğŸ“‹ Final diagnostics:"
          kubectl get pod ${{ env.MONGODB_POD }} --namespace=${{ env.KUBE_NAMESPACE }}
          kubectl describe pod ${{ env.MONGODB_POD }} --namespace=${{ env.KUBE_NAMESPACE }}
          kubectl logs ${{ env.MONGODB_POD }} --namespace=${{ env.KUBE_NAMESPACE }} --tail=50
          exit 1
        fi
        
        # Final verification
        echo "ğŸ”’ Final MongoDB connectivity check..."
        timeout 10 bash -c 'while ! nc -z localhost ${{ env.MONGODB_PORT }}; do sleep 0.5; done' || {
          echo "âŒ Final verification failed - MongoDB not accessible"
          exit 1
        }
        echo "âœ… MongoDB is ready and accessible on localhost:${{ env.MONGODB_PORT }}"
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Cache dependencies
      uses: actions/cache@v4
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt', '**/pyproject.toml') }}
        restore-keys: |
          ${{ runner.os }}-pip-
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements/base.txt
        pip install -r requirements/test.txt
    
    - name: Run unit tests
      env:
        MONGODB_URI: mongodb://localhost:27017
        S3_BUCKET_NAME: test-bucket
        AZURE_STORAGE_ACCOUNT: test-account
        GCP_PROJECT_ID: test-project
      run: |
        pytest tests/test_api.py -v --cov=src/backend --cov-report=xml
    
    - name: Run integration tests
      env:
        MONGODB_URI: mongodb://localhost:27017
      run: |
        pytest tests/test_integration.py -v
    
    - name: ğŸ§¹ Cleanup Kubernetes Resources
      if: always()
      run: |
        echo "ğŸ§¹ Cleaning up Kubernetes resources..."
        
        # Kill port-forward process using stored PID
        if [ -f /tmp/mongodb-port-forward-${{ github.run_id }}.pid ]; then
          PID=$(cat /tmp/mongodb-port-forward-${{ github.run_id }}.pid)
          echo "ğŸ“Œ Killing port-forward process with PID: $PID"
          kill $PID 2>/dev/null || true
          rm -f /tmp/mongodb-port-forward-${{ github.run_id }}.pid
        fi
        
        # Also kill any remaining port-forward processes for this namespace (fallback)
        pkill -f "kubectl port-forward.*${{ env.MONGODB_POD }}.*${{ env.KUBE_NAMESPACE }}" || true
        
        # Clean up MongoDB pod
        echo "ğŸ—‘ï¸ Deleting MongoDB pod..."
        kubectl delete pod ${{ env.MONGODB_POD }} \
          --namespace=${{ env.KUBE_NAMESPACE }} \
          --ignore-not-found=true \
          --grace-period=5
        
        # Clean up namespace (this removes all resources in the namespace)
        echo "ğŸ—‘ï¸ Deleting namespace ${{ env.KUBE_NAMESPACE }}..."
        kubectl delete namespace ${{ env.KUBE_NAMESPACE }} --ignore-not-found=true &
        
        echo "âœ… Cleanup initiated (namespace deletion continues in background)"
    
    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        flags: unittests
        name: codecov-umbrella
        fail_ci_if_error: false

  lint:
    name: Lint Code
    runs-on: [self-hosted, containerd]
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install linting tools
      run: |
        python -m pip install --upgrade pip
        pip install flake8 black isort mypy
    
    - name: Run Black
      run: black --check src/ tests/
      continue-on-error: true
    
    - name: Run isort
      run: isort --check-only src/ tests/
      continue-on-error: true
    
    - name: Run Flake8
      run: flake8 src/ tests/ --max-line-length=120 --ignore=E203,W503
      continue-on-error: true
    
    - name: Run MyPy
      run: mypy src/ --ignore-missing-imports
      continue-on-error: true

  # Container build using nerdctl (containerd-native)
  container-build:
    name: ğŸ³ Build Container Images
    runs-on: [self-hosted, containerd]
    
    steps:
    - uses: actions/checkout@v4
    
    - name: ğŸ—ï¸ Build Backend Image with nerdctl
      run: |
        echo "ğŸ—ï¸ Building backend image with nerdctl..."
        nerdctl build -t speecher-backend:ci-${{ github.run_id }} .
        echo "âœ… Backend image built successfully"
    
    - name: ğŸ—ï¸ Build Frontend Image with nerdctl
      run: |
        echo "ğŸ—ï¸ Building frontend image with nerdctl..."
        if [ -f "docker/react.Dockerfile" ]; then
          nerdctl build -f docker/react.Dockerfile -t speecher-frontend:ci-${{ github.run_id }} .
        else
          echo "âš ï¸ Frontend Dockerfile not found, skipping frontend build"
        fi
        echo "âœ… Frontend build completed"
    
    - name: ğŸ§ª Test Container Functionality
      run: |
        echo "ğŸ§ª Testing backend container..."
        
        # Create test namespace
        kubectl create namespace test-${{ github.run_id }} || true
        
        # Deploy backend for testing
        kubectl run speecher-backend-test \
          --image=speecher-backend:ci-${{ github.run_id }} \
          --namespace=test-${{ github.run_id }} \
          --port=8000 \
          --restart=Never \
          --env="ENVIRONMENT=test"
        
        # Wait for pod to be ready
        echo "â³ Waiting for backend pod to be ready..."
        kubectl wait --for=condition=ready pod/speecher-backend-test \
          --timeout=60s \
          --namespace=test-${{ github.run_id }} || {
            echo "âš ï¸ Backend pod not ready, checking logs..."
            kubectl logs speecher-backend-test --namespace=test-${{ github.run_id }} --tail=20 || true
            kubectl get pods --namespace=test-${{ github.run_id }}
          }
        
        # Port forward with retry logic
        echo "ğŸ”— Setting up port forwarding for backend test..."
        BACKEND_PORT_FORWARD_PID=""
        PORT_FORWARD_SUCCESS=false
        
        for attempt in {1..3}; do
          echo "ğŸ“¡ Port forwarding attempt $attempt/3..."
          
          # Kill any existing port-forward
          pkill -f "kubectl port-forward.*speecher-backend-test.*test-${{ github.run_id }}" || true
          sleep 1
          
          # Start port forwarding
          kubectl port-forward pod/speecher-backend-test 8000:8000 \
            --namespace=test-${{ github.run_id }} &
          BACKEND_PORT_FORWARD_PID=$!
          echo $BACKEND_PORT_FORWARD_PID > /tmp/backend-port-forward-${{ github.run_id }}.pid
          
          # Wait and verify
          sleep 3
          
          if nc -z -w 1 localhost 8000 2>/dev/null; then
            echo "âœ… Backend port 8000 is accessible!"
            PORT_FORWARD_SUCCESS=true
            break
          else
            echo "âš ï¸ Port 8000 not accessible on attempt $attempt"
            kill $BACKEND_PORT_FORWARD_PID 2>/dev/null || true
            [ $attempt -lt 3 ] && sleep 3
          fi
        done
        
        if [ "$PORT_FORWARD_SUCCESS" = true ]; then
          # Test health endpoint
          echo "ğŸ” Testing health endpoint..."
          curl -f http://localhost:8000/health || echo "âš ï¸ Health check failed (may be expected for some setups)"
        else
          echo "âš ï¸ Could not establish port forwarding to backend container"
        fi
        
        # Cleanup
        if [ -f /tmp/backend-port-forward-${{ github.run_id }}.pid ]; then
          kill $(cat /tmp/backend-port-forward-${{ github.run_id }}.pid) 2>/dev/null || true
          rm -f /tmp/backend-port-forward-${{ github.run_id }}.pid
        fi
        pkill -f "kubectl port-forward.*speecher-backend-test" || true
        kubectl delete namespace test-${{ github.run_id }} --ignore-not-found=true &
        
        echo "âœ… Container functionality test completed"

  security:
    name: Security Scan
    runs-on: [self-hosted, containerd]
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install security tools
      run: |
        python -m pip install --upgrade pip
        pip install safety bandit
    
    - name: Run Safety check
      run: |
        pip install -r requirements/base.txt || true
        safety check || true
      continue-on-error: true
    
    - name: Run Bandit
      run: bandit -r src/ -f json -o bandit-report.json
      continue-on-error: true
    
    - name: Upload security reports
      uses: actions/upload-artifact@v4
      with:
        name: security-reports
        path: |
          bandit-report.json
        retention-days: 30

  deploy:
    name: ğŸš€ Deploy to Production
    needs: [test-optimized, test, lint, security, container-build]  # Includes both test paths
    runs-on: [self-hosted, containerd]
    if: github.ref == 'refs/heads/main' && github.event_name == 'push'
    
    steps:
    - uses: actions/checkout@v4
    
    - name: ğŸš€ Deploy Notification
      run: |
        echo "ğŸ‰ Ready for deployment!"
        echo "ğŸ“¦ Container images built: speecher-backend:ci-${{ github.run_id }}"
        echo "ğŸ”§ This is where you would deploy to production"
        echo "ğŸ’¡ Add your K8s deployment steps here (kubectl apply, Helm, ArgoCD, etc.)"
    
    # Kubernetes-native deployment examples (uncomment and configure)
    # - name: ğŸš€ Deploy to Kubernetes
    #   run: |
    #     # Tag images for production
    #     nerdctl tag speecher-backend:ci-${{ github.run_id }} speecher-backend:latest
    #     
    #     # Push to registry (if configured)
    #     # nerdctl push your-registry/speecher-backend:latest
    #     
    #     # Deploy using kubectl
    #     # kubectl apply -f k8s/production/
    #     
    #     # Or deploy using Helm
    #     # helm upgrade --install speecher ./helm-chart --namespace=production
    
    # - name: ğŸ³ Push to Container Registry
    #   env:
    #     REGISTRY_USERNAME: ${{ secrets.REGISTRY_USERNAME }}
    #     REGISTRY_PASSWORD: ${{ secrets.REGISTRY_PASSWORD }}
    #   run: |
    #     # Login to your container registry
    #     echo $REGISTRY_PASSWORD | nerdctl login -u $REGISTRY_USERNAME --password-stdin your-registry.com
    #     
    #     # Tag and push images
    #     nerdctl tag speecher-backend:ci-${{ github.run_id }} your-registry.com/speecher-backend:latest
    #     nerdctl push your-registry.com/speecher-backend:latest