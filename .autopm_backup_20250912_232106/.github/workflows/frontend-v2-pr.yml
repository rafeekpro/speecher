name: Frontend v2 PR Checks

on:
  pull_request:
    branches: [develop, main, feature/frontend-v2]
    paths:
      - 'src/frontend/**'
      - 'src/react-frontend/**'
      - 'tests/frontend/**'
      - 'package.json'
      - 'package-lock.json'
      - 'tsconfig.json'
      - '.github/workflows/frontend-v2-pr.yml'

permissions:
  contents: read
  issues: write
  pull-requests: write

env:
  # Frontend-specific environment variables
  FRONTEND_DIR: './src/react-frontend'
  # Use existing github-runner namespace for all operations
  KUBE_NAMESPACE: 'github-runner'

jobs:
  test-and-build:
    runs-on: [self-hosted, playwright, e2e]
    
    strategy:
      matrix:
        node-version: [18.x, 20.x]
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Setup Node.js ${{ matrix.node-version }}
      uses: actions/setup-node@v4
      with:
        node-version: ${{ matrix.node-version }}
        cache: 'npm'
        cache-dependency-path: 'src/react-frontend/package-lock.json'
    
    - name: Install dependencies
      run: npm ci
      working-directory: ./src/react-frontend
    
    - name: Run linting (ESLint via react-scripts)
      run: npx eslint src/ --ext .js,.jsx --max-warnings 0 || echo "Linting completed with warnings"
      working-directory: ./src/react-frontend
      continue-on-error: true
    
    - name: Skip type checking (JavaScript project)
      run: echo "Skipping TypeScript check - this is a JavaScript project"
      working-directory: ./src/react-frontend
    
    - name: Run tests with coverage
      run: npm test -- --coverage --watchAll=false --passWithNoTests
      working-directory: ./src/react-frontend
      env:
        CI: true
    
    - name: Upload coverage to Codecov
      if: matrix.node-version == '20.x'
      uses: codecov/codecov-action@v3
      with:
        file: ./src/react-frontend/coverage/lcov.info
        flags: frontend
        name: frontend-coverage
    
    - name: Build application
      run: npm run build
      working-directory: ./src/react-frontend
      env:
        CI: true
    
    - name: Check bundle size
      if: matrix.node-version == '20.x'
      working-directory: ./src/react-frontend
      run: |
        echo "## ğŸ“¦ Bundle Size Report" >> $GITHUB_STEP_SUMMARY
        echo "\`\`\`" >> $GITHUB_STEP_SUMMARY
        du -sh dist/* 2>/dev/null || du -sh build/* 2>/dev/null || echo "Build output not found" >> $GITHUB_STEP_SUMMARY
        echo "\`\`\`" >> $GITHUB_STEP_SUMMARY
    
    - name: Comment PR with test results
      if: github.event_name == 'pull_request' && matrix.node-version == '20.x'
      uses: actions/github-script@v7
      with:
        script: |
          const fs = require('fs');
          let coverageData = {};
          
          try {
            if (fs.existsSync('./src/react-frontend/coverage/coverage-summary.json')) {
              coverageData = JSON.parse(fs.readFileSync('./src/react-frontend/coverage/coverage-summary.json', 'utf8'));
            }
          } catch (error) {
            console.log('Coverage data not found');
          }
          
          const total = coverageData.total || {};
          const statements = total.statements || {};
          const branches = total.branches || {};
          const functions = total.functions || {};
          const lines = total.lines || {};
          
          const comment = `## ğŸ“Š Test Coverage Report
          
          | Metric | Coverage | Status |
          |--------|----------|--------|
          | Statements | ${statements.pct || 'N/A'}% | ${statements.pct >= 80 ? 'âœ…' : 'âš ï¸'} |
          | Branches | ${branches.pct || 'N/A'}% | ${branches.pct >= 80 ? 'âœ…' : 'âš ï¸'} |
          | Functions | ${functions.pct || 'N/A'}% | ${functions.pct >= 80 ? 'âœ…' : 'âš ï¸'} |
          | Lines | ${lines.pct || 'N/A'}% | ${lines.pct >= 80 ? 'âœ…' : 'âš ï¸'} |
          
          âœ… All checks passed for Node ${{ matrix.node-version }}`;
          
          // Find existing comment
          const { data: comments } = await github.rest.issues.listComments({
            owner: context.repo.owner,
            repo: context.repo.repo,
            issue_number: context.issue.number,
          });
          
          const botComment = comments.find(comment => 
            comment.user.type === 'Bot' && 
            comment.body.includes('Test Coverage Report')
          );
          
          if (botComment) {
            await github.rest.issues.updateComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              comment_id: botComment.id,
              body: comment
            });
          } else {
            await github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });
          }

  security-check:
    runs-on: [self-hosted, linux, x64, kubernetes]
    steps:
    - uses: actions/checkout@v4
    
    - name: Run security audit
      run: |
        npm ci || npm install
        npm audit --audit-level=critical --omit=dev || echo "âš ï¸ Security audit found issues in dev dependencies"
      working-directory: ./src/react-frontend
      continue-on-error: true
    
    - name: Check for secrets (basic scan)
      run: |
        echo "ğŸ” Scanning for potential secrets in changed files..."
        
        # Basic check for common secret patterns without Docker
        if git diff --name-only ${{ github.event.pull_request.base.sha }}..${{ github.event.pull_request.head.sha }} | xargs -I {} grep -l -i -E "(api[_-]?key|secret|password|token)" {} 2>/dev/null; then
          echo "âš ï¸ Found files with potential secrets - please review manually"
        else
          echo "âœ… No obvious secret patterns found in changed files"
        fi
      continue-on-error: true

  container-build:
    name: ğŸ³ Frontend Container Build
    runs-on: [self-hosted, linux, x64, kubernetes]
    needs: [test-and-build]
    if: success()  # Only run if tests pass
    
    steps:
    - uses: actions/checkout@v4
    
    - name: ğŸ“¦ Build Frontend Application
      run: |
        echo "ğŸ“¦ Building frontend application..."
        cd ${{ env.FRONTEND_DIR }}
        npm ci
        npm run build
        echo "âœ… Frontend build completed"
      working-directory: ./
    
    - name: ğŸ—ï¸ Build Frontend Container with Kaniko
      run: |
        echo "ğŸ—ï¸ Building frontend container for PR ${{ github.event.pull_request.number }}..."
        
        # Use the optimized Dockerfile from the repo instead of inline
        echo "ğŸ“„ Using optimized production Dockerfile..."
        
        # Create ConfigMap with optimized Dockerfile and nginx config
        kubectl create configmap kaniko-dockerfile-${{ github.run_id }} \
          --from-file=Dockerfile=./docker/react.Dockerfile \
          --from-file=nginx.prod.conf=./docker/nginx.prod.conf \
          --namespace=${{ env.KUBE_NAMESPACE }}
        
        # NO tarball or ConfigMap for source code - we'll use Git clone in init container
        
        # Build with Kaniko in Kubernetes
        echo "ğŸ”¨ Building container image with Kaniko..."
        
        # Clean up any existing Kaniko job and related ConfigMaps from previous runs
        echo "ğŸ§¹ Cleaning up any existing Kaniko resources..."
        kubectl delete job kaniko-frontend-${{ github.run_id }} --namespace=${{ env.KUBE_NAMESPACE }} --ignore-not-found=true
        kubectl delete configmap kaniko-dockerfile-${{ github.run_id }} --namespace=${{ env.KUBE_NAMESPACE }} --ignore-not-found=true
        
        # Create optimized Kaniko build job
        cat <<EOF | kubectl apply -f -
        apiVersion: batch/v1
        kind: Job
        metadata:
          name: kaniko-frontend-${{ github.run_id }}
          namespace: ${{ env.KUBE_NAMESPACE }}
          labels:
            app: kaniko-build
            build-type: container
            component: frontend
        spec:
          backoffLimit: 2
          activeDeadlineSeconds: 600
          ttlSecondsAfterFinished: 3600  # Auto-cleanup after 1 hour
          template:
            metadata:
              labels:
                app: kaniko-build
                component: frontend
            spec:
              restartPolicy: Never
              serviceAccountName: github-runner  # Use dedicated ServiceAccount
              initContainers:
              - name: git-clone
                image: alpine/git:latest
                command: ['sh', '-c']
                args:
                  - |
                    echo "Cloning repository..."
                    # Clone the repository at the specific commit
                    git clone --depth 10 \
                      https://github.com/${{ github.repository }}.git /workspace
                    
                    cd /workspace
                    # Checkout the exact commit for this PR
                    git checkout ${{ github.sha }}
                    
                    # Copy Dockerfile and nginx config from ConfigMap
                    cp /config/Dockerfile /workspace/Dockerfile
                    cp /config/nginx.prod.conf /workspace/nginx.prod.conf
                    
                    echo "Setting permissions for user 1000..."
                    chown -R 1000:1000 /workspace
                    
                    # Verify structure for multi-stage build
                    echo "Build context structure:"
                    ls -la /workspace/
                    ls -la /workspace/src/react-frontend/ || echo "react-frontend directory will be created during build"
                    find /workspace -type f -name "*.json" | head -5
                volumeMounts:
                - name: workspace
                  mountPath: /workspace
                - name: dockerfile-config
                  mountPath: /config
                securityContext:
                  runAsUser: 0  # Git needs root to clone
                  allowPrivilegeEscalation: false
              containers:
              - name: kaniko
                image: gcr.io/kaniko-project/executor:latest
                args:
                - --dockerfile=/workspace/Dockerfile
                - --context=dir:///workspace
                - --destination=speecher-frontend:pr-${{ github.event.pull_request.number }}
                - --no-push
                - --cache=true
                - --cache-ttl=24h
                - --cache-repo=speecher-frontend-cache
                - --cache-copy-layers=true
                - --compressed-caching=false
                - --use-new-run=true
                - --verbosity=info
                env:
                - name: DOCKER_CONFIG
                  value: /tmp/.docker
                volumeMounts:
                - name: workspace
                  mountPath: /workspace
                resources:
                  requests:
                    memory: "1Gi"
                    cpu: "500m"
                  limits:
                    memory: "2Gi"
                    cpu: "1000m"
                securityContext:
                  runAsUser: 1000
                  runAsNonRoot: true
                  allowPrivilegeEscalation: false
                  readOnlyRootFilesystem: true
                  capabilities:
                    drop:
                    - ALL
              volumes:
              - name: workspace
                emptyDir: {}
              - name: dockerfile-config
                configMap:
                  name: kaniko-dockerfile-${{ github.run_id }}
        EOF
        
        # Wait for build
        kubectl wait --for=condition=complete job/kaniko-frontend-${{ github.run_id }} \
          --timeout=600s \
          --namespace=${{ env.KUBE_NAMESPACE }} || {
          echo "âŒ Kaniko build failed or timed out"
          echo "ğŸ“‹ Fetching Kaniko logs..."
          kubectl logs job/kaniko-frontend-${{ github.run_id }} --namespace=${{ env.KUBE_NAMESPACE }} --all-containers=true --tail=100
          kubectl delete job kaniko-frontend-${{ github.run_id }} --namespace=${{ env.KUBE_NAMESPACE }} --ignore-not-found=true
          kubectl delete configmap kaniko-dockerfile-${{ github.run_id }} --namespace=${{ env.KUBE_NAMESPACE }} --ignore-not-found=true
          exit 1
        }
        
        echo "âœ… Frontend container build completed"
        
        # Cleanup
        kubectl delete job kaniko-frontend-${{ github.run_id }} --namespace=${{ env.KUBE_NAMESPACE }} --ignore-not-found=true
        kubectl delete configmap kaniko-dockerfile-${{ github.run_id }} --namespace=${{ env.KUBE_NAMESPACE }} --ignore-not-found=true
    
    - name: ğŸ” Debug Pause - Kubernetes Cluster Inspection
      if: failure()
      run: |
        echo "ğŸ” ========================================"
        echo "ğŸ” TEMPORARY DEBUG PAUSE - CLUSTER INSPECTION"
        echo "ğŸ” ========================================"
        echo ""
        echo "âŒ Build failed! Pausing for 15 minutes to allow manual debugging."
        echo ""
        echo "ğŸ“‹ Current Kubernetes cluster state:"
        echo "------------------------------------"
        
        # Show current cluster state
        echo "ğŸ” Pods in namespace ${{ env.KUBE_NAMESPACE }}:"
        kubectl get pods --namespace=${{ env.KUBE_NAMESPACE }} -o wide --sort-by=.metadata.creationTimestamp
        echo ""
        
        echo "ğŸ” Jobs in namespace ${{ env.KUBE_NAMESPACE }}:"
        kubectl get jobs --namespace=${{ env.KUBE_NAMESPACE }} -o wide --sort-by=.metadata.creationTimestamp
        echo ""
        
        echo "ğŸ” ConfigMaps related to this build:"
        kubectl get configmaps --namespace=${{ env.KUBE_NAMESPACE }} | grep -E "${{ github.run_id }}|dockerfile" || echo "No build-related ConfigMaps found"
        echo ""
        
        echo "ğŸ› ï¸  MANUAL DEBUGGING COMMANDS:"
        echo "-----------------------------"
        echo "Run these commands locally to debug further:"
        echo ""
        echo "# Connect to the same cluster context:"
        echo "kubectl config current-context"
        echo ""
        echo "# Check pods and jobs:"
        echo "kubectl get pods --namespace=${{ env.KUBE_NAMESPACE }} -o wide"
        echo "kubectl get jobs --namespace=${{ env.KUBE_NAMESPACE }} -o wide"
        echo ""
        echo "# Find the failed Kaniko pod:"
        echo "kubectl get pods --namespace=${{ env.KUBE_NAMESPACE }} -l app=kaniko-build,component=frontend"
        echo ""
        echo "# Check init container logs (git-clone):"
        echo "kubectl logs <pod-name> --namespace=${{ env.KUBE_NAMESPACE }} -c git-clone"
        echo ""
        echo "# Check main container logs (kaniko):"
        echo "kubectl logs <pod-name> --namespace=${{ env.KUBE_NAMESPACE }} -c kaniko"
        echo ""
        echo "# Get detailed pod description:"
        echo "kubectl describe pod <pod-name> --namespace=${{ env.KUBE_NAMESPACE }}"
        echo ""
        echo "# Check job description:"
        echo "kubectl describe job kaniko-frontend-${{ github.run_id }} --namespace=${{ env.KUBE_NAMESPACE }}"
        echo ""
        echo "# Check events:"
        echo "kubectl get events --namespace=${{ env.KUBE_NAMESPACE }} --sort-by=.lastTimestamp"
        echo ""
        echo "# Check ConfigMap contents:"
        echo "kubectl get configmap kaniko-dockerfile-${{ github.run_id }} --namespace=${{ env.KUBE_NAMESPACE }} -o yaml"
        echo ""
        echo "â° DEBUG TIMEOUT: This step will sleep for 15 minutes (900 seconds)"
        echo "   Use this time to run the commands above and investigate the issue."
        echo ""
        echo "ğŸ”¥ To skip the remaining sleep time, cancel this workflow run."
        echo ""
        echo "â° Starting 15-minute debug pause..."
        
        # Sleep for 15 minutes to allow manual investigation
        for i in {1..15}; do
          echo "â° Debug pause: $i/15 minutes elapsed..."
          sleep 60
        done
        
        echo ""
        echo "âœ… Debug pause completed. Check the logs above for debugging commands."
        echo "ğŸ” ========================================"
    
    - name: ğŸ” Debug Frontend Kaniko Build Failure
      if: failure()
      run: |
        echo "ğŸ” ====================================="
        echo "ğŸ” FRONTEND KANIKO BUILD FAILURE DIAGNOSTICS"
        echo "ğŸ” ====================================="
        
        # Configuration for this workflow
        JOB_NAME="kaniko-frontend-${{ github.run_id }}"
        NAMESPACE="${{ env.KUBE_NAMESPACE }}"
        BUILD_TYPE="frontend"
        
        echo "ğŸ” Job Name: $JOB_NAME"
        echo "ğŸ” Namespace: $NAMESPACE"
        echo "ğŸ” Build Type: $BUILD_TYPE"
        echo ""
        
        # Step 1: Find the Kaniko pod using job selectors
        echo "ğŸ“‹ Step 1: Finding Kaniko pod..."
        echo "==============================="
        
        KANIKO_POD=""
        
        # Method 1: Find by job-name label
        KANIKO_POD=$(kubectl get pods -n $NAMESPACE \
          -l job-name=$JOB_NAME \
          -o jsonpath='{.items[0].metadata.name}' 2>/dev/null || echo "")
        
        if [[ -n "$KANIKO_POD" ]]; then
          echo "âœ… Found pod by job-name label: $KANIKO_POD"
          KANIKO_POD_EXISTS=true
        else
          # Method 2: Find by app=kaniko-build label
          KANIKO_POD=$(kubectl get pods -n $NAMESPACE \
            -l app=kaniko-build,component=frontend \
            -o jsonpath='{.items[0].metadata.name}' 2>/dev/null || echo "")
          
          if [[ -n "$KANIKO_POD" ]]; then
            echo "âœ… Found pod by app label: $KANIKO_POD"
            KANIKO_POD_EXISTS=true
          else
            echo "âŒ No Kaniko pod found!"
            KANIKO_POD_EXISTS=false
          fi
        fi
        
        if [[ "$KANIKO_POD_EXISTS" == "false" ]]; then
          echo "ğŸ“‹ Available pods in namespace $NAMESPACE:"
          kubectl get pods -n $NAMESPACE --sort-by=.metadata.creationTimestamp
          echo ""
          echo "ğŸ“‹ Available jobs in namespace $NAMESPACE:"
          kubectl get jobs -n $NAMESPACE --sort-by=.metadata.creationTimestamp
        fi
        echo ""
        
        # Step 2: Job Status and Events
        echo "ğŸ“‹ Step 2: Job Status and Events"
        echo "================================"
        
        echo "ğŸ” Job description:"
        kubectl describe job $JOB_NAME -n $NAMESPACE 2>/dev/null || {
          echo "âŒ Job $JOB_NAME not found"
          echo "ğŸ“‹ Available jobs:"
          kubectl get jobs -n $NAMESPACE -o wide | grep -E "(kaniko|frontend|build)" || echo "No build jobs found"
        }
        echo ""
        
        echo "ğŸ” Job events:"
        kubectl get events -n $NAMESPACE \
          --field-selector involvedObject.name=$JOB_NAME \
          --sort-by='.lastTimestamp' 2>/dev/null || echo "âŒ No job events found"
        echo ""
        
        # Step 3: Pod Diagnostics (if pod exists)
        if [[ "$KANIKO_POD_EXISTS" == "true" ]]; then
          echo "ğŸ“‹ Step 3: Pod Diagnostics"
          echo "=========================="
          
          echo "ğŸ” Pod description:"
          kubectl describe pod $KANIKO_POD -n $NAMESPACE 2>/dev/null || echo "âŒ Pod description failed"
          echo ""
          
          echo "ğŸ” Pod status:"
          kubectl get pod $KANIKO_POD -n $NAMESPACE -o wide 2>/dev/null || echo "âŒ Pod status failed"
          echo ""
          
          echo "ğŸ” Pod events:"
          kubectl get events -n $NAMESPACE \
            --field-selector involvedObject.name=$KANIKO_POD \
            --sort-by='.lastTimestamp' 2>/dev/null || echo "âŒ No pod events found"
          echo ""
        fi
        
        # Step 4: Container Logs
        echo "ğŸ“‹ Step 4: Container Logs"
        echo "========================="
        
        if [[ "$KANIKO_POD_EXISTS" == "true" ]]; then
          echo "ğŸ” Init container logs (prepare-build-context):"
          echo "------------------------------------------------"
          kubectl logs $KANIKO_POD -n $NAMESPACE -c prepare-build-context --tail=50 2>/dev/null || {
            echo "âŒ No init container logs available"
          }
          echo ""
          
          echo "ğŸ” Kaniko container logs:"
          echo "-------------------------"
          kubectl logs $KANIKO_POD -n $NAMESPACE -c kaniko --tail=100 2>/dev/null || {
            echo "âŒ No Kaniko container logs available"
            echo "ğŸ” Trying to get logs from any container..."
            kubectl logs $KANIKO_POD -n $NAMESPACE --all-containers=true --tail=50 2>/dev/null || \
              echo "âŒ Could not retrieve any container logs"
          }
          echo ""
        else
          echo "âš ï¸ No pod found - checking job logs directly"
          kubectl logs job/$JOB_NAME -n $NAMESPACE --tail=100 2>/dev/null || echo "âŒ No job logs available"
          echo ""
        fi
        
        # Step 5: ConfigMaps and Build Context
        echo "ğŸ“‹ Step 5: Build Context and ConfigMaps"
        echo "======================================="
        
        echo "ğŸ” ConfigMaps related to this build:"
        kubectl get configmaps -n $NAMESPACE -o name | grep -E "${{ github.run_id }}" | head -5 || \
          echo "âŒ No build-related ConfigMaps found"
        
        # Check if Dockerfile ConfigMap exists
        if kubectl get configmap kaniko-dockerfile-${{ github.run_id }} -n $NAMESPACE >/dev/null 2>&1; then
          echo "âœ… Dockerfile ConfigMap exists"
        else
          echo "âŒ Dockerfile ConfigMap missing"
        fi
        
        # Check if Dockerfile ConfigMap exists
        if kubectl get configmap kaniko-dockerfile-${{ github.run_id }} -n $NAMESPACE >/dev/null 2>&1; then
          echo "âœ… Dockerfile ConfigMap exists"
        else
          echo "âŒ Dockerfile ConfigMap missing"
        fi
        echo ""
        
        # Step 6: Troubleshooting Hints for Frontend
        echo "ğŸ“‹ Step 6: Frontend-Specific Troubleshooting"
        echo "==========================================="
        
        echo "ğŸ”§ Common frontend build failures:"
        echo "1. âŒ Node.js build process failed"
        echo "   - Check package.json and dependencies"
        echo "   - Verify build scripts and environment variables"
        echo ""
        echo "2. âŒ Multi-stage Docker build issues"
        echo "   - Check if react.Dockerfile exists and is valid"
        echo "   - Verify nginx configuration"
        echo ""
        echo "3. âŒ Build context preparation failed"
        echo "   - Check if source files were properly extracted"
        echo "   - Verify tarball creation and extraction"
        echo ""
        echo "4. âŒ Resource constraints"
        echo "   - Frontend builds can be memory-intensive"
        echo "   - Check if build exceeded memory/CPU limits"
        echo ""
        
        echo "ğŸ” =============================="
        echo "ğŸ” END FRONTEND FAILURE ANALYSIS"
        echo "ğŸ” =============================="
    
    - name: ğŸ§ª Test Frontend Container
      run: |
        echo "ğŸ§ª Testing frontend container..."
        
        # Test the built container by running it briefly
        echo "ğŸ” Testing container functionality..."
        
        # Run container test in Kubernetes with enhanced security
        cat <<EOF | kubectl apply -f -
        apiVersion: v1
        kind: Pod
        metadata:
          name: frontend-test-${{ github.run_id }}
          namespace: ${{ env.KUBE_NAMESPACE }}
          labels:
            app: frontend-test
            component: frontend
            pr: "${{ github.event.pull_request.number }}"
        spec:
          restartPolicy: Never
          serviceAccountName: github-runner
          containers:
          - name: frontend
            image: speecher-frontend:pr-${{ github.event.pull_request.number }}
            imagePullPolicy: Never
            ports:
            - containerPort: 8080
            env:
            - name: NODE_ENV
              value: "production"
            resources:
              requests:
                memory: "128Mi"
                cpu: "100m"
              limits:
                memory: "256Mi"
                cpu: "200m"
            securityContext:
              runAsUser: 1000
              runAsNonRoot: true
              allowPrivilegeEscalation: false
              readOnlyRootFilesystem: true
              capabilities:
                drop:
                - ALL
        EOF
        
        if kubectl get pod frontend-test-${{ github.run_id }} --namespace=${{ env.KUBE_NAMESPACE }} 2>/dev/null; then
          echo "ğŸ“¦ Test pod started"
          
          # Wait for pod to be ready
          kubectl wait --for=condition=ready pod/frontend-test-${{ github.run_id }} \
            --timeout=60s --namespace=${{ env.KUBE_NAMESPACE }} || true
          
          # Setup port forwarding to the correct port
          kubectl port-forward pod/frontend-test-${{ github.run_id }} 8080:8080 \
            --namespace=${{ env.KUBE_NAMESPACE }} &
          PF_PID=$!
          sleep 5
          
          # Test if container is responding with comprehensive checks
          echo "ğŸ” Testing container health..."
          sleep 10  # Allow more time for container startup
          
          # Test health endpoint
          if curl -s -f http://localhost:8080/health > /dev/null; then
            echo "âœ… Health check passed"
            HEALTH_STATUS="success"
          else
            echo "âš ï¸ Health check failed"
            HEALTH_STATUS="warning"
          fi
          
          # Test main application
          if curl -s -f http://localhost:8080 | grep -q "<title>"; then
            echo "âœ… Frontend application responding with HTML"
            APP_STATUS="success"
          else
            echo "âš ï¸ Frontend application not responding correctly"
            APP_STATUS="warning"
          fi
          
          # Overall status
          if [ "$HEALTH_STATUS" = "success" ] && [ "$APP_STATUS" = "success" ]; then
            CONTAINER_STATUS="success"
          else
            CONTAINER_STATUS="warning"
          fi
          
          # Get container logs for debugging
          echo "ğŸ“‹ Container logs:"
          kubectl logs frontend-test-${{ github.run_id }} --namespace=${{ env.KUBE_NAMESPACE }} --tail=10 || true
          
          # Stop port forwarding and cleanup
          kill $PF_PID 2>/dev/null || true
          kubectl delete pod frontend-test-${{ github.run_id }} --namespace=${{ env.KUBE_NAMESPACE }} --ignore-not-found=true
          
          if [ "$CONTAINER_STATUS" = "success" ]; then
            echo "âœ… Frontend container test completed successfully"
          else
            echo "âš ï¸ Frontend container test completed with warnings"
          fi
        else
          echo "âŒ Failed to start frontend container"
          exit 1
        fi

  label-pr:
    runs-on: [self-hosted, linux, x64, kubernetes]
    if: github.event_name == 'pull_request'
    steps:
    - uses: actions/labeler@v4
      with:
        repo-token: "${{ secrets.GITHUB_TOKEN }}"
        configuration-path: .github/labeler.yml